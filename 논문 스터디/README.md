|날짜|제목|학습 내용|
|----|----|----|
|220225|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)|BERT 논문|
|220218|[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)|Transformer 코드 분석|
|220211|[Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)|Transformer 논문|
|220114 - 220204||word2vec 코드 구현|
|220107|[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)|word2vec 논문|
|211231|[Efficient Estimation of Word Representations in Vector space](https://arxiv.org/pdf/1301.3781.pdf)|word2vec 논문|
