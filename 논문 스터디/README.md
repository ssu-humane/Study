논문 스터디
====================
NLP의 기초 신경망 구조에 대한 논문을 읽고 각 신경망 방법의 motivation과 원리, 그리고 수학적 표현에 대해 이해해보는 스터디를 진행하였습니다.

- 스터디 진행 방식
     - 논문 : 제시된 신경망 방법의 motivation과 원리, 그리고 수학적 표현에 대해 설명할 수 있도록 이해
     - 코드 : 어떤입력이 어떤 구조를거쳐 어떤 차원의 정보로 변환되는지 그 과정을 분석
     - 모든 랩원이 논문 읽기/ 코드 분석을 하고 스터디에 참여하며, 랜덤으로 발표자를 뽑아 진행


|날짜|제목|학습 내용|
|----|----|----|
|220225|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)|BERT 논문|
|220218|[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)|Transformer 코드 분석|
|220211|[Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)|Transformer 논문|
|220114 - 220204||word2vec 코드 구현|
|220107|[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)|word2vec 논문|
|211231|[Efficient Estimation of Word Representations in Vector space](https://arxiv.org/pdf/1301.3781.pdf)|word2vec 논문|
