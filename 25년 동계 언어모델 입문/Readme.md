Stanford CS 324
============================
<br><br>
Lecture
|주차|날짜|내용|발표자|
|------|---|---|---|
|1|[241227](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Lecture/20241227_CS%20324%20chapter1%2C2_%EB%B0%95%ED%98%84%EB%B9%88.pptx.pdf)|[Introduction, Capabilities](https://stanford-cs324.github.io/winter2022/lectures/introduction/)|박현빈|
|2|[250103](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Lecture/20250103_CS324%20Chapter%203%2C%204_%EC%B5%9C%EC%A2%85%ED%98%84.pptx.pdf)|[Harms I, Harms II](https://stanford-cs324.github.io/winter2022/lectures/harms-1/)|최종현|
|3|[250110](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Lecture/20250110_CS324_Data%2C%20Legal%20Considerations_%EA%B9%80%ED%83%9C%EA%B7%A0.pdf)|[Data, Legal considerations](https://stanford-cs324.github.io/winter2022/lectures/data/)|김태균|
|4|[250117](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Lecture/20250117_CS324_Modeling_Training_%EA%B9%80%EA%B1%B4%EC%88%98.pdf)|[Modeling, Training](https://stanford-cs324.github.io/winter2022/lectures/modeling/)|김건수|
|5|[250131](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Lecture/20250131_CS324_Modular%20architectures_%EA%B9%80%ED%83%9C%EA%B7%A0.pdf)|[Modular architectures](https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/)|김태균|
|6|[250207](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Lecture/20250207_CS324_Adaptation_%E1%84%80%E1%85%B5%E1%86%B7%E1%84%90%E1%85%A2%E1%84%80%E1%85%B2%E1%86%AB.pdf)|[Adaptation](https://stanford-cs324.github.io/winter2022/lectures/adaptation/)|김태균|
<br>

<br><br>
Discussion Paper
|주차|날짜|내용|발표자|
|------|---|---|---|
|1|[241227](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20241227%20On%20the%20Opportunities%20and%20Risks%20of%20Foundation%20Models_%EC%B5%9C%EC%A2%85%ED%98%84.pdf)|[On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258)|최종현|
|2|[250103](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250103_REALTOXICITYPROMPTS_Evaluating%20Neural%20Toxic%20Degeneration%20in%20Language%20Models_%EA%B3%A0%EA%B2%BD%EB%B9%88.pptx.pdf)|[REALTOXICITYPROMPTS:Evaluating Neural Toxic Degeneration in Language Modles](https://arxiv.org/pdf/2009.11462)|고경빈|
|2|[250103](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250103%20On%20the%20Dangers%20of%20Stochastic%20Parrots%20-%20Can%20Language%20Models%20Be%20Too%20Big_%EA%B9%80%ED%83%9C%EA%B7%A0.pdf)|[On the Dangers of Stochastic Parrots:Can Language Models Be too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)|김태균|
|3|[250110](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250110_CS324_The%20Pile%20An%20800GB%20Dataset%20of%20Diverse%20Text%20for%20Language%20Modeling_%EB%B0%95%ED%98%84%EB%B9%88.pdf)|[The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/pdf/2101.00027)|박현빈|
|4|[250117](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250110_Fair%20Learning_%EC%B5%9C%EC%A2%85%ED%98%84.pdf)|[Fair learning](https://texaslawreview.org/fair-learning/)|최종현|
|5|[250124](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/250124_DeepSpeed_Extreme-scale%20model%20training%20for%20everyone_%EA%B9%80%ED%83%9C%EA%B7%A0.pdf)|[DeepSpeed](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)|김태균|
|5|[250124](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250124_Scaling_Laws_for_Neural_Language_model_%EA%B9%80%EA%B1%B4%EC%88%98.pdf)|[Scaling laws](https://arxiv.org/pdf/2001.08361)|김건수|
|6|[250131](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250131_Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks_%E1%84%87%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A7%E1%86%AB%E1%84%87%E1%85%B5%E1%86%AB.pdf)|[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)|박현빈|
|7|[250207](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250207_Parameter-Efficient-Prompt-Tuning_%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%A5%E1%86%AB%E1%84%89%E1%85%AE.pdf)|[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)|김건수|
|8|[250228](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250228_Instruction%20tuning_Alpaca_%E1%84%80%E1%85%B5%E1%86%B7%E1%84%90%E1%85%A2%E1%84%80%E1%85%B2%E1%86%AB.pdf)|[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)|김태균|
|8|[250228](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250228_Orca%2C%20Progressive%20Learning%20from%20Complex%20Explanation%20Traces%20of%20GPT-4_%E1%84%87%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A7%E1%86%AB%E1%84%87%E1%85%B5%E1%86%AB.pdf)|[Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707)|박현빈|
|9|[250307](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250306_LoRA_%E1%84%8E%E1%85%AC%E1%84%8C%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A7%E1%86%AB.pdf)|[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)|최종현|
|9|[250307](https://github.com/ssu-humane/Study/blob/main/25%EB%85%84%20%EB%8F%99%EA%B3%84%20%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%20%EC%9E%85%EB%AC%B8/Discussion%20paper/20250306_QLoRA_%E1%84%92%E1%85%A5%E1%84%8B%E1%85%B2%E1%86%AB%E1%84%89%E1%85%A5.pdf)|[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)|허윤서|
<br>

